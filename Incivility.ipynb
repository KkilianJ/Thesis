{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPPnAf1O/DGuuvyhDzKOAsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KkilianJ/Thesis/blob/main/Incivility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168ZiiZyuqv2",
        "outputId": "501aca8e-b3fc-4372-c3b4-262fb115c7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#with punctuation\n",
        "df_long = pd.read_csv('/content/drive/MyDrive/Thesis/long_text_with_pos_text1.csv', low_memory = False)\n",
        "text_long = df_long['bigram_text'].tolist()"
      ],
      "metadata": {
        "id": "DFrpwfbevENg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_short= pd.read_csv('/content/drive/MyDrive/Thesis/short_text_with_pos_text1.csv', low_memory = False)\n",
        "text_short = df_short['bigram_text'].tolist()"
      ],
      "metadata": {
        "id": "Cixe5s-M1bBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import statistics\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "d0ERiXij65Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncivil_lexicon = {}\n",
        "with open('/content/drive/MyDrive/Thesis/incivilities.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 3 and parts[1] == 'UNCIV':\n",
        "            word = parts[0].lower()\n",
        "            score = float(parts[2])\n",
        "            uncivil_lexicon[word] = score"
      ],
      "metadata": {
        "id": "KA16Yo5d7pPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma(text):\n",
        "    return \" \".join([token.lemma_ for token in nlp(text)])\n",
        "\n",
        "def clean_punct(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)"
      ],
      "metadata": {
        "id": "9Zz64pjj7slG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uncivil_word_sentence(sentence, uncivil_dict):\n",
        "    tokens = sentence.split()\n",
        "    count = sum(1 for word in tokens if word in uncivil_dict)\n",
        "    score = sum(uncivil_dict.get(word, 0) for word in tokens)\n",
        "    length = len(tokens)\n",
        "    return (count / length if length > 0 else 0, score / length if length > 0 else 0)\n",
        "\n",
        "def uncivil_word_tweet(tweet, uncivil_dict):\n",
        "    tokens = tweet.split()\n",
        "    count = sum(1 for word in tokens if word in uncivil_dict)\n",
        "    score = sum(uncivil_dict.get(word, 0) for word in tokens)\n",
        "    length = len(tokens)\n",
        "    return (count / length if length > 0 else 0, score / length if length > 0 else 0)\n",
        "\n",
        "def process(text):\n",
        "    lem = lemma(text)\n",
        "    doc = nlp(lem)\n",
        "    sents = list(doc.sents)\n",
        "\n",
        "    sentence_props = [uncivil_word_sentence(sent.text, uncivil_lexicon) for sent in sents]\n",
        "    if sentence_props:\n",
        "        per_sentence = sum(p[0] for p in sentence_props) / len(sentence_props)\n",
        "        per_sentence_score = sum(p[1] for p in sentence_props) / len(sentence_props)\n",
        "    else:\n",
        "        per_sentence = per_sentence_score = 0\n",
        "\n",
        "    cleaned = clean_punct(lem)\n",
        "    per_tweet, per_tweet_score = uncivil_word_tweet(cleaned, uncivil_lexicon)\n",
        "\n",
        "    return (per_sentence, per_sentence_score, per_tweet, per_tweet_score)\n"
      ],
      "metadata": {
        "id": "Sj2F3hN57wzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(texts, workers=42):\n",
        "    with Pool(processes=workers) as pool:\n",
        "        results = list(tqdm(pool.imap_unordered(process, texts), total=len(texts)))\n",
        "    return results\n",
        "\n",
        "def chunk_gogogo(df, text_column, chunk_size=50000, workers=42):\n",
        "    results_all = []\n",
        "    for i in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[i:i+chunk_size].copy()\n",
        "        texts = chunk[text_column].tolist()\n",
        "        results = process_chunk(texts, workers=workers)\n",
        "        chunk[['per_sentence', 'per_sentence_score', 'per_tweet', 'per_tweet_score']] = results\n",
        "        results_all.append(chunk)\n",
        "    final_df = pd.concat(results_all, ignore_index=True)\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "1bg8gH4E71Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_df_long = chunk_gogogo(df_long, text_column=\"bigram_text\", chunk_size=50000, workers=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPySjyst8jEU",
        "outputId": "b4ac87af-ede4-4430-cb50-9dc0efd399aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1208.17it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1203.57it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1206.94it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1205.13it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.52it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.28it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.89it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1230.53it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.26it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.34it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.66it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.24it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.55it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1230.14it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.08it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.95it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1232.12it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.14it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1221.66it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.25it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1224.67it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1222.90it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1214.94it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1219.58it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1226.13it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1224.78it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1221.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1224.15it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1224.05it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.23it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.14it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.51it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.20it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1221.38it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.81it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1223.52it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1220.52it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:41<00:00, 1218.02it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.97it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1229.25it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1230.86it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1226.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.10it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1223.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1223.38it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1226.12it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.53it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.48it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.34it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1223.19it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1226.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1226.99it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.62it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1228.20it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1227.27it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1221.85it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1225.16it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49988/49988 [00:40<00:00, 1228.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final_df_long.to_csv('/content/drive/MyDrive/Thesis/long_text_with_pos_text1.csv', index=False)\n",
        "#print(final_df_long)"
      ],
      "metadata": {
        "id": "4gYvRPbz0KLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cupy as cp\n",
        "from cuml.cluster import KMeans as cuKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cols = ['per_sentence', 'per_sentence_score', 'per_tweet', 'per_tweet_score']\n",
        "df_long = df_long.dropna(subset=cols)\n",
        "X = df_long[cols].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_gpu = cp.asarray(X_scaled)\n",
        "\n",
        "kmeans = cuKMeans(n_clusters=2, random_state=42)\n",
        "df_long['cluster'] = kmeans.fit_predict(X_gpu).get()\n",
        "\n",
        "sampled_df = df_long.groupby('cluster').apply(lambda x: x.sample(50, random_state=42)).reset_index(drop=True)\n",
        "sampled_df['id'] = ['sampled_' + str(i).zfill(4) for i in range(len(sampled_df))]\n",
        "sampled_df = sampled_df[['id', 'text', 'cluster']]\n",
        "sampled_shuffled = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "sampled_shuffled['text'].to_csv('/content/drive/MyDrive/Thesis/gold_label_Long.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKO4P0EL3b2K",
        "outputId": "7a60ebd0-0dc1-49b9-addf-96255537bc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:195: FutureWarning: The default value of `n_init` will change from 1 to 'auto' in 25.04. Set the value of `n_init` explicitly to suppress this warning.\n",
            "  return func(*args, **kwargs)\n",
            "/tmp/ipython-input-39-1474280217.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = df_long.groupby('cluster').apply(lambda x: x.sample(50, random_state=42)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0 is civil 1 is uncivil\n",
        "gold_label1 =\n",
        "  [\n",
        "    1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
        "    1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
        "    0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
        "    1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
        "    1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1\n",
        "  ]\n",
        "\n",
        "#1 is civil 0 is uncivil\n",
        "gold_label2 =\n",
        "  [0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
        " 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
        " 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
        " 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
        " 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0\n",
        "   ]\n",
        "\n",
        "report1 = classification_report(gold_label1, cluster_labels, target_names=[\"Uncivil\", \"Civil\"])\n",
        "print(report1)\n",
        "report1 = classification_report(gold_label2, cluster_labels, target_names=[\"Uncivil\", \"Civil\"])\n",
        "print(report1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhV-YkxbAQGR",
        "outputId": "ded0119b-2ac6-45cd-8efa-54c12ab01bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Uncivil       0.76      0.70      0.73        54\n",
            "       Civil       0.68      0.74      0.71        46\n",
            "\n",
            "    accuracy                           0.72       100\n",
            "   macro avg       0.72      0.72      0.72       100\n",
            "weighted avg       0.72      0.72      0.72       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "final_df_short = chunk_gogogo(df_short, text_column=\"bigram_text\", chunk_size=50000, workers=42)\n",
        "final_df_short.to_csv('/content/drive/MyDrive/Thesis/short_text_with_pos_text1.csv', index=False)\n",
        "print(final_df_short)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyLU_UOL1s57",
        "outputId": "3731cfce-db59-44e1-cbfb-2996c15f49e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 392.98it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:06<00:00, 396.51it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:05<00:00, 399.07it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.81it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.39it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:02<00:00, 406.86it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.15it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.73it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:02<00:00, 407.93it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:02<00:00, 407.07it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:02<00:00, 406.51it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:02<00:00, 406.87it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.72it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.44it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:06<00:00, 394.88it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:06<00:00, 394.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 393.07it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 392.55it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 391.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 391.88it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:07<00:00, 390.93it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.91it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.53it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.85it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.10it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:04<00:00, 402.50it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:04<00:00, 403.09it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.29it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.68it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.09it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.05it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.20it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.58it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.99it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.01it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.04it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.93it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.98it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.05it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.60it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.35it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.84it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.80it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.20it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.89it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.41it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.36it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.73it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.90it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 405.36it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.58it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.25it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.23it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 404.48it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:03<00:00, 403.43it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49984/49984 [02:03<00:00, 404.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      text  viewCount  \\\n",
            "0        @GOP @SheehyforMT Interesting perspective! But...        3.0   \n",
            "1        @co_rapunzel4 @MSNBC @nbc @CBS @ABC @CNN I hav...       16.0   \n",
            "2        @Mayor_Steinberg I want to share my blood and ...       81.0   \n",
            "3        @ErrataRob @GeorgeOu Correct, the gop makes it...       36.0   \n",
            "4        @BidensWins Are you going to change your Twitt...        3.0   \n",
            "...                                                    ...        ...   \n",
            "2999979  Lord Hannan highlights issues with the Conserv...      109.0   \n",
            "2999980  @JoeBiden I love this! ðŸ™ðŸ½â¤ï¸ðŸ˜¥ðŸ’™ðŸ’™ðŸ’™ðŸ’™Thank you. Pre...        4.0   \n",
            "2999981  PresidentÂ Joe BidenÂ should not miss this chanc...        9.0   \n",
            "2999982  @MaryDou80139756 @aurorabrshealis @VP She was ...       49.0   \n",
            "2999983  @EdKrassen Did you just abandon his majesty Jo...        8.0   \n",
            "\n",
            "         likeCount  quoteCount  replyCount  retweetCount  char_count  \\\n",
            "0              0.0         0.0         0.0           0.0         238   \n",
            "1              0.0         0.0         0.0           0.0         132   \n",
            "2              0.0         0.0         0.0           0.0         276   \n",
            "3              0.0         0.0         0.0           0.0          95   \n",
            "4              0.0         0.0         0.0           0.0          95   \n",
            "...            ...         ...         ...           ...         ...   \n",
            "2999979        0.0         0.0         1.0           0.0         143   \n",
            "2999980        0.0         0.0         0.0           0.0          63   \n",
            "2999981        0.0         0.0         0.0           0.0         256   \n",
            "2999982        1.0         0.0         1.0           0.0         130   \n",
            "2999983        0.0         0.0         0.0           0.0          69   \n",
            "\n",
            "                                               bigram_text  per_sentence  \\\n",
            "0        interesting perspective! but instead of labeli...      0.307692   \n",
            "1        i haven't asked for either. i saw trump get sh...      0.800000   \n",
            "2        i want to share my blood and tears with people...      0.227273   \n",
            "3        correct, the gop makes it seem like illegal vo...      0.444444   \n",
            "4        are you going to change your twitter account t...      0.352941   \n",
            "...                                                    ...           ...   \n",
            "2999979  lord hannan highlights issues with the conserv...      0.517778   \n",
            "2999980          i love this! â¤ï¸thank you. president biden      0.606061   \n",
            "2999981  president joe biden should not miss this chanc...      0.646309   \n",
            "2999982  she was appointed (by pres biden) as lead or c...      0.720000   \n",
            "2999983  did you just abandon his majesty joe biden, di...      0.542189   \n",
            "\n",
            "         per_sentence_score  per_tweet  per_tweet_score  \n",
            "0                 -0.003246   0.363636        -0.003837  \n",
            "1                  0.024112   0.800000         0.024112  \n",
            "2                 -0.022434   0.500000        -0.049355  \n",
            "3                 -0.039223   0.571429        -0.050430  \n",
            "4                 -0.013736   0.400000        -0.015568  \n",
            "...                     ...        ...              ...  \n",
            "2999979           -0.009051   0.673913        -0.027334  \n",
            "2999980           -0.023332   0.625000        -0.024061  \n",
            "2999981           -0.024758   0.806452        -0.029625  \n",
            "2999982           -0.013920   0.750000        -0.014500  \n",
            "2999983           -0.008662   0.653846        -0.020801  \n",
            "\n",
            "[2999984 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Text**"
      ],
      "metadata": {
        "id": "zhBgVEY_CnU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import field_size_limit\n",
        "import pandas as pd\n",
        "import cupy as cp\n",
        "from cuml.cluster import KMeans as cuKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "file_path = '/content/drive/MyDrive/Thesis/short_text_with_pos_text1.csv'\n",
        "df_final_short = pd.read_csv(file_path, low_memory= False)\n",
        "cols = ['per_sentence', 'per_sentence_score', 'per_tweet', 'per_tweet_score']\n",
        "df_short = df_final_short.dropna(subset=cols)\n",
        "X = df_short[cols].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_gpu = cp.asarray(X_scaled)\n",
        "\n",
        "kmeans = cuKMeans(n_clusters=2, random_state=42)\n",
        "df_short['cluster'] = kmeans.fit_predict(X_gpu).get()\n",
        "\n",
        "sampled_df = df_short.groupby('cluster').apply(lambda x: x.sample(50, random_state=42)).reset_index(drop=True)\n",
        "sampled_df['id'] = ['sampled_' + str(i).zfill(4) for i in range(len(sampled_df))]\n",
        "sampled_df = sampled_df[['id', 'text', 'cluster']]\n",
        "sampled_shuffled = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "sampled_shuffled['text'].to_csv('/content/drive/MyDrive/Thesis/gold_label_Short.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvX3r_s1Woqt",
        "outputId": "8125328d-382e-421f-9e58-dac8ef0efb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:195: FutureWarning: The default value of `n_init` will change from 1 to 'auto' in 25.04. Set the value of `n_init` explicitly to suppress this warning.\n",
            "  return func(*args, **kwargs)\n",
            "/tmp/ipython-input-11-930619374.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = df_short.groupby('cluster').apply(lambda x: x.sample(50, random_state=42)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0 is civil 1 is uncivil\n",
        "gold_label1 =  [\n",
        "    0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
        "    0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
        "    1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
        "    0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
        "    0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
        "    0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
        "    1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
        "    0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
        "    0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
        "    1, 0, 0, 1, 0, 1, 0, 1, 1, 1\n",
        "]\n",
        "\n",
        "#1 is civil 0 is uncivil\n",
        "gold_label2 = [\n",
        "1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
        "1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
        "0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
        "1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
        "1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
        "1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
        "0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
        "1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
        "1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
        "0, 1, 1, 0, 1, 0, 1, 0, 0, 0\n",
        "\n",
        "]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "cluster_labels = sampled_shuffled['cluster'].tolist()\n",
        "\n",
        "report2 = classification_report(gold_label1, cluster_labels, target_names=[\"Uncivil\", \"Civil\"])\n",
        "print(report2)\n",
        "report2 = classification_report(gold_label2, cluster_labels, target_names=[\"Uncivil\", \"Civil\"])\n",
        "print(report2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpvgxsWkW5ps",
        "outputId": "d3196f39-6456-4b17-bcca-ebe11316a7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Uncivil       0.74      0.53      0.62        70\n",
            "       Civil       0.34      0.57      0.42        30\n",
            "\n",
            "    accuracy                           0.54       100\n",
            "   macro avg       0.54      0.55      0.52       100\n",
            "weighted avg       0.62      0.54      0.56       100\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Uncivil       0.26      0.43      0.33        30\n",
            "       Civil       0.66      0.47      0.55        70\n",
            "\n",
            "    accuracy                           0.46       100\n",
            "   macro avg       0.46      0.45      0.44       100\n",
            "weighted avg       0.54      0.46      0.48       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Machine Learning Failed**\n",
        "**Problem 1**: The dataset is severely imbalanced â€” the number of civil tweets is approximately 30 times greater than uncivil ones (around 3,000 vs. 189).\n",
        "\n",
        "**Problem 2**: The paper claims that 6,000 rows were manually labeled. However, neither of the datasets provided (CLAPTON_augmented.csv and Twitter Deliberative Politics.csv) actually contains 6,000 labeled entries. In fact, most of the labels present appear to be incorrect. It seems that the authors may have referred to the Twitter Deliberative Politics.csv file, assuming the full dataset was hand-labeled, but this is inconsistent with what I observed.\n",
        "\n",
        "**Problem 3**: Given the poor label quality, I find the classification results reported in the paper to be unreliable. The large number of incorrect or missing labels seriously undermines the dataset's validity for supervised machine learning tasks. Model perform super bad on their own data, consistently below 15% on classifying uncivil message.\n",
        "\n",
        "I replicated the model training process as described in the article, but the performance was significantly below expectations. Even after incorporating weighted features for uncivil sentences and tweets, there was no meaningful improvement in the results.\n",
        "\n",
        "I experimented with various models, including Random Forest, MLP, Bayesian and Logistic Regression. However, the precision for classifying uncivil tweets consistently remained below 15%, which I consider unacceptably low.\n",
        "\n",
        "The research that I replicate is: https://doi.org/10.1093/joc/jqz023\n",
        "According to the sentence, we operationalized incivility in terms of the daily number of uncivil tweets or the average percentage proportion of uncivil words per tweet, as predicted by different machine learning and dictionary-based lexical methods.\n",
        "\n",
        "I used SMOTE to reduce the sample imbalanced effect on machine learning.\n",
        "\n",
        "features that I have try before: (all below 15% on classifying uncivil tweets)\n",
        "1. percentages based on tweets and text\n",
        "2. percentages based on tweets and text, each sentence score and each tweets score\n",
        "3. word embedded * tf-idf and per based on tweets and text or percentages based on tweets and text, each sentence score and each tweets score\n",
        "4.tf-idf to vectorize tweets and percentages based on tweets and text or percentages based on tweets and text, each sentence score and each tweets score"
      ],
      "metadata": {
        "id": "I6Fr719yvAU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_long.to_csv(\"/content/drive/MyDrive/Thesis/long_text_with_pos_text1.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Ssnh8QfAJsif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Modelling\n",
        "paper_path = '/content/drive/MyDrive/Thesis/Twitter Deliberative Politics.csv'\n",
        "df_paper = pd.read_csv(paper_path, encoding='ISO-8859-1',low_memory = False)\n",
        "\n",
        "text = df_paper['message'].tolist()\n",
        "\n",
        "\n",
        "#text preprocessing\n",
        "import re\n",
        "\n",
        "def preprocessing(text_list):\n",
        "    cleaned = []\n",
        "    for text in text_list:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "        text = re.sub(r\"#\\w+\", \"\", text)\n",
        "        text = re.sub(r\"@\\w+\", \"\", text)\n",
        "        text = re.sub(r\"<f0><u\\+\\d{4}><u\\+\\d{4}><u\\+\\d{4}>\", \"\", text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r\"<u\\+\\d{4}>\", \"\", text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r\"<f0>\", \"\", text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'^[^\\w\\s]+', '', text) #the punctuation comes first\n",
        "        text = re.sub(r'^-+\\s*', '', text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        cleaned.append(text)\n",
        "    return cleaned\n",
        "\n",
        "df_paper['message_cleaned'] = preprocessing(df_paper['message'].fillna(\"\").astype(str).tolist())"
      ],
      "metadata": {
        "id": "gTn3_V7XJ-4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_paper = chunk_gogogo(df_paper, text_column=\"message_cleaned\", chunk_size=50000, workers=42)\n",
        "final_df_paper.to_csv(\"/content/drive/MyDrive/Thesis/Twitter Deliberative Politics.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZQ2bS2HMiae",
        "outputId": "3c77287c-f2e1-4aee-d5db-d33b61907d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5585/5585 [00:02<00:00, 2260.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df_paper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugmxPrR5jkJ8",
        "outputId": "33336e06-1816-45d2-d1ce-0e8c7c209058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      message_id                                            message  \\\n",
            "0              1  @USER- #GrahamCassidy will devastate #Military...   \n",
            "1              2  @USER- The US people &amp; Minnesotans must se...   \n",
            "2              4  =@USER - \"we all want the same thing when you ...   \n",
            "3              5  @USER - A poison in our island - Rising seas c...   \n",
            "4              6  =@USER - hypocrite. You are A porn surfer and ...   \n",
            "...          ...                                                ...   \n",
            "5580        5984           @USER/please stand up for our democracy!   \n",
            "5581        5986   @USER: $3,124,273 from the NRA during her career   \n",
            "5582        5987  @USER: Glad you bribed well to cheat, lie, sch...   \n",
            "5583        5988  @USER: Stand By Your Ad should be applied to o...   \n",
            "5584        5989  @USER: You must call for the appointment of a ...   \n",
            "\n",
            "      Constructiveness  Justification  Justification_internal  \\\n",
            "0                    0              1                       0   \n",
            "1                    0              1                       0   \n",
            "2                    0              1                       0   \n",
            "3                    0              1                       0   \n",
            "4                    0              0                       0   \n",
            "...                ...            ...                     ...   \n",
            "5580                 0              0                       0   \n",
            "5581                 0              1                       0   \n",
            "5582                 0              0                       0   \n",
            "5583                 0              1                       0   \n",
            "5584                 0              0                       0   \n",
            "\n",
            "      Justification_external  Relevance  Reciprocity  Empathy_Respect  \\\n",
            "0                          0        1.0          1.0              1.0   \n",
            "1                          0        1.0          1.0              1.0   \n",
            "2                          0        1.0          NaN              1.0   \n",
            "3                          0        1.0          0.0              NaN   \n",
            "4                          0        NaN          0.0              0.0   \n",
            "...                      ...        ...          ...              ...   \n",
            "5580                       0        1.0          0.0              1.0   \n",
            "5581                       0        1.0          0.0              NaN   \n",
            "5582                       0        1.0          0.0              0.0   \n",
            "5583                       0        1.0          0.0              0.0   \n",
            "5584                       0        1.0          0.0              NaN   \n",
            "\n",
            "      Uncivil_abuse                                    message_cleaned  \\\n",
            "0               1.0  will devastate w/ kids like justin who need . ...   \n",
            "1               1.0  the us people &amp; minnesotans must see the s...   \n",
            "2               NaN  - \"we all want the same thing when you look at...   \n",
            "3               0.0  - a poison in our island - rising seas caused ...   \n",
            "4               NaN  - hypocrite. you are a porn surfer and claim t...   \n",
            "...             ...                                                ...   \n",
            "5580            0.0                 please stand up for our democracy!   \n",
            "5581            0.0          $3,124,273 from the nra during her career   \n",
            "5582            NaN  glad you bribed well to cheat, lie, scheme. yo...   \n",
            "5583            0.0  stand by your ad should be applied to online a...   \n",
            "5584            0.0  you must call for the appointment of a special...   \n",
            "\n",
            "      per_sentence  per_tweet  per_sentence_score  per_tweet_score  \n",
            "0         0.000000   0.000000            0.000000         0.000000  \n",
            "1         0.000000   0.000000            0.000000         0.000000  \n",
            "2         0.000000   0.000000            0.000000         0.000000  \n",
            "3         0.000000   0.000000            0.000000         0.000000  \n",
            "4         0.000000   0.000000            0.000000         0.000000  \n",
            "...            ...        ...                 ...              ...  \n",
            "5580      0.543651   0.650000           -0.020435        -0.023398  \n",
            "5581      0.580000   0.805556           -0.014115        -0.027142  \n",
            "5582      0.550725   0.538462           -0.024586        -0.029261  \n",
            "5583      0.641946   0.760870           -0.030030        -0.035582  \n",
            "5584      0.569951   0.705882           -0.031807        -0.038139  \n",
            "\n",
            "[5585 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Thesis/Twitter Deliberative Politics.csv').dropna(subset=['Uncivil_abuse','per_sentence', 'per_sentence_score','per_tweet', 'per_tweet_score'])\n",
        "\n",
        "X = df[['per_sentence', 'per_sentence_score', 'per_tweet', 'per_tweet_score']].values\n",
        "y = df['Uncivil_abuse'].astype(int).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfaCOQDUVsIH",
        "outputId": "8c077362-b2c5-4c56-cfc4-97484866cf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.47795414462081126\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.47      0.61       491\n",
            "           1       0.14      0.54      0.22        76\n",
            "\n",
            "    accuracy                           0.48       567\n",
            "   macro avg       0.50      0.50      0.41       567\n",
            "weighted avg       0.77      0.48      0.56       567\n",
            "\n"
          ]
        }
      ]
    }
  ]
}